### huggingface에서 pruning할 모델을 composer로 바꿔준다. 

# Define the Hugging Face model name and the output path
HF_MODEL_NAME=meta-llama/Llama-2-7b-hf   ## huggingface args 
OUTPUT_PATH=models/Llama-2-7b-composer/state_dict.pt ### output path

# Create the necessary directory if it doesn't exist
mkdir -p $(dirname $OUTPUT_PATH)

# Convert the Hugging Face model to Composer key format
python3 -m llmshearing.utils.composer_to_hf $HF_MODEL_NAME $OUTPUT_PATH False

### pruning 진행

bash ./llmshearing/scripts/pruning.sh

### pruning 모델을 hf로 바꾼다

MODEL_PATH=../outputs/llama2_7b_pruning_scaling_doremi_to1.3b_sl2048/latest-rank0.pt
python3 -m llmshearing.utils.post_pruning_processing prune_and_save_model $MODEL_PATH

MODEL_DIR=/../outputs/llama2_7b_pruning_scaling_doremi_to1.3b_sl4096
MODEL_PATH=$MODEL_DIR/pruned-latest-rank0.pt
OUTPUT_PATH=$MODEL_DIR/hf-latest_rank0
MODEL_CLASS=LlamaForCausalLM
HIDDEN_SIZE=2048
NUM_ATTENTION_HEADS=16
NUM_HIDDEN_LAYERS=24
INTERMEDIATE_SIZE=5504
MODEL_NAME=Sheared-Llama-1.3B_ds

python3 -m llmshearing.utils.composer_to_hf $MODEL_PATH $OUTPUT_PATH True \
        model_class=${MODEL_CLASS} \
        hidden_size=${HIDDEN_SIZE} \
        num_attention_heads=${NUM_ATTENTION_HEADS} \
        num_hidden_layers=${NUM_HIDDEN_LAYERS} \
        intermediate_size=${INTERMEDIATE_SIZE} \
        num_key_value_heads=${NUM_ATTENTION_HEADS} \
        _name_or_path=${MODEL_NAME}


### eval을 한다. (lm-harness)

bash /home/dongseok/pruning/LLM-Shearing-main/icl_eval/run_eval_new.sh
